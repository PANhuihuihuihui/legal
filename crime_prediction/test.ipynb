{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,default_data_collator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-511fb7c719a2c1cb\n",
      "Reusing dataset json (/home/huijie/.cache/huggingface/datasets/json/default-511fb7c719a2c1cb/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f310f53c5c4513b6291ccbbbe50054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=29, names=['盜竊罪', '爆竊罪', '猥褻侵犯罪', '侵害人身罪', '不小心駕駛罪', '販運危險藥物罪', '入屋犯法罪', '搶劫罪', '以欺騙手段取得財產罪', '販毒罪', '販運毒品罪', '普通襲擊罪', '洗黑錢罪', '串謀詐騙罪', '處理贓物罪', '刑事恐嚇罪', '串謀勒索罪', '非法入境罪', '強姦罪', '非禮罪', '縱火罪', '扒竊罪', '管有危險藥物罪', '危險駕駛引致他人死亡罪', '襲擊致造成身體傷害罪', '管有虛假文書罪', '使用虛假文書罪', '危險駕駛引致他人身體受嚴重傷害罪', '危險駕駛罪'], id=None),\n",
       " 'begin': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "features = Features({\"text\": Value(\"string\"),\"label\":ClassLabel(num_classes = 29,names=['盜竊罪', '爆竊罪', '猥褻侵犯罪', '侵害人身罪', '不小心駕駛罪', '販運危險藥物罪', '入屋犯法罪', \n",
    "            '搶劫罪', '以欺騙手段取得財產罪', '販毒罪', '販運毒品罪', '普通襲擊罪', '洗黑錢罪', '串謀詐騙罪', \n",
    "            '處理贓物罪', '刑事恐嚇罪', '串謀勒索罪', '非法入境罪', '強姦罪', '非禮罪', '縱火罪', '扒竊罪', \n",
    "            '管有危險藥物罪', '危險駕駛引致他人死亡罪', '襲擊致造成身體傷害罪', '管有虛假文書罪', '使用虛假文書罪',\n",
    "             '危險駕駛引致他人身體受嚴重傷害罪', '危險駕駛罪']) ,\"begin\": Value(\"string\")})\n",
    "dataset = load_dataset(\"json\", data_files=\"/home/huijie/legal/crime_prediction/data_prepare/crime_prediction.json\",features=features)\n",
    "dataset = dataset[\"train\"]\n",
    "dataset.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/huijie/.cache/huggingface/datasets/json/default-511fb7c719a2c1cb/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-6849bb085e66de49.arrow\n"
     ]
    }
   ],
   "source": [
    "labels = ['盜竊罪', '爆竊罪', '猥褻侵犯罪', '侵害人身罪', '不小心駕駛罪', '販運危險藥物罪', '入屋犯法罪', \n",
    "            '搶劫罪', '以欺騙手段取得財產罪', '販毒罪', '販運毒品罪', '普通襲擊罪', '洗黑錢罪', '串謀詐騙罪', \n",
    "            '處理贓物罪', '刑事恐嚇罪', '串謀勒索罪', '非法入境罪', '強姦罪', '非禮罪', '縱火罪', '扒竊罪', \n",
    "            '管有危險藥物罪', '危險駕駛引致他人死亡罪', '襲擊致造成身體傷害罪', '管有虛假文書罪', '使用虛假文書罪',\n",
    "             '危險駕駛引致他人身體受嚴重傷害罪', '危險駕駛罪']\n",
    "label2id =  {k:v for v,k in enumerate(labels)}\n",
    "dataset = dataset.align_labels_with_mapping(label2id, \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['方第一證人需要透過手機作出紀錄。本席裁定控方第一證人當時正在依法執行公務。  陳大律師陳述，控方未能在毫無合理疑點下，證明被告的行為構成阻礙。  高等法院原訟法庭暫委法官黃崇厚（當時官階），在香港特別行政區訴尹明義 案指出，《簡易程序治罪條例》第23條和《侵害人身罪條例》第36（b）條的罪行不同，因為《簡易程序治罪條例》的條文中，沒有故意（wilful）這字眼，所以控方毋須證明被告人故意阻礙執行公務，意即毋須證明阻礙執行公務是被告人進行某行為的意圖（intention）。以《簡易程序治罪條例》第23條這罪行來說，控方',\n",
       "  ' 沒有任何法例賦予被告權力，容許他當時自行蒐證或執法，因此，縱使他真誠相信他人違反私穩法例，亦不可以强搶他人物品。立法會議員並沒此方面的權力。本席找不到任何法理支持被告搶手機的行為。被告的行為肯定是非法的武力。  基於前述，控方已在毫無合理疑點下，證明所有普通襲擊罪的控罪元素。本席裁定此項控罪罪名成立。 阻礙公職人員執行公務  根據《釋義及通則條例》第3條，公職人員（public officer）指任何在特區政府擔任受薪職位的人。控方第一證人是保安局高級行政主任，必然是公職人員。本席接納案發當日該小組負責在立法會大樓'],\n",
       " 'label': [3, 11],\n",
       " 'begin': ['許智峯是立法會議員，可以進入位於中環立法會道1號的立法會綜合大樓，履行公務。 2018年4月24日，立法會大樓會議室1內正舉行《廣深港高鐵條例草案》委員會會議。 當日，政府人員組成一個小組，成員包括保安局高級行政主任梁諾施，及六名運輸及房屋局的人員。他們在立法會大樓不同位置候命，並記錄立法會議員在草案委員會會議舉行時的行蹤。運輸及房屋局助理秘書長運輸3B黎惠珊，是該小組的組長，負責監察及統籌該小組組員；運輸及房屋局助理秘書長運輸3C鄭朗峰，是該小組統籌員；運輸及房屋局二級系統分析／程序編製主任運輸1梁榮燊，',\n",
       "  '許智峯是立法會議員，可以進入位於中環立法會道1號的立法會綜合大樓，履行公務。 2018年4月24日，立法會大樓會議室1內正舉行《廣深港高鐵條例草案》委員會會議。 當日，政府人員組成一個小組，成員包括保安局高級行政主任梁諾施，及六名運輸及房屋局的人員。他們在立法會大樓不同位置候命，並記錄立法會議員在草案委員會會議舉行時的行蹤。運輸及房屋局助理秘書長運輸3B黎惠珊，是該小組的組長，負責監察及統籌該小組組員；運輸及房屋局助理秘書長運輸3C鄭朗峰，是該小組統籌員；運輸及房屋局二級系統分析／程序編製主任運輸1梁榮燊，']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer([examples[\"begin\"],examples[\"text\"]],truncation = True,max_length = 512)\n",
    "    # if tokenizer.is_fast:\n",
    "    #     result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    result[\"input_ids\"] = result[\"input_ids\"][0]+ result[\"input_ids\"][1]\n",
    "    result[\"attention_mask\"] = result[\"attention_mask\"][0]+ result[\"attention_mask\"][1]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "        tokenize_function, batched=False, remove_columns=[\"begin\",\"text\"],num_proc =16\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 27,\n",
       " 28}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenized_datasets['label'][:200])\n",
    "# tokenized_datasets.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2id = {'B': 0, 'C': 1}\n",
    "# tokenized_datasets.align_labels_with_mapping(label2id, \"a_similar_to\")\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"classification\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer,padding= \"max_length\",max_length = 512,pad_to_multiple_of= 8)\n",
    "# train_dataloader = DataLoader(tokenized_datasets,8,collate_fn=data_collator)\n",
    "train_dataloader = DataLoader(tokenized_datasets,8,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(inputs: Tensor, targets: Tensor,device,\n",
    "                alpha: float = 4.0,beta: float= .0,reduction: str='mean',\n",
    "                eval: bool = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "    targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "            (-1 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "\n",
    "    prediction = None\n",
    "    if eval:\n",
    "        prediction = (torch.sigmoid(inputs.mul(alpha).add(beta)) > 0.5).float()\n",
    "    print(inputs.shape,targets.shape)\n",
    "    loss = F.logsigmoid(inputs.mul(targets).mul(alpha).add(beta))\n",
    "    loss = loss.sum(dim = -1).div(-alpha) # b \n",
    "    if reduction == 'mean':\n",
    "        loss = loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        loss = loss.sum()\n",
    "    return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]])\n",
      "torch.Size([8, 29]) torch.Size([8])\n",
      "tensor([[-6.4583e-01, -3.9616e-01,  9.1491e-01, -3.9861e-01,  6.7614e-01,\n",
      "          1.3132e+00,  2.7350e+00, -6.1841e-01,  6.2908e-01, -9.9037e-01,\n",
      "         -5.8056e-01,  5.8771e-01,  1.8531e+00,  3.1409e-01, -7.4116e-01,\n",
      "          8.0280e-01,  4.1807e-01, -1.3559e+00, -5.6186e-01, -1.3086e-01,\n",
      "          1.2159e+00,  1.5746e-01, -7.0076e-01,  2.8799e-01, -4.8290e-01,\n",
      "         -4.3749e-02,  1.0259e+00, -5.0461e-02, -1.7445e+00],\n",
      "        [-8.1304e-02,  4.7145e-01,  1.2877e+00, -2.1891e-01,  1.8952e+00,\n",
      "          5.8965e-01, -5.2007e-01, -1.7082e+00, -6.6966e-01,  4.7454e-01,\n",
      "          1.1245e-01, -6.2055e-01, -4.2170e-01,  1.0966e+00, -6.3484e-01,\n",
      "         -5.2962e-02,  1.3077e+00,  1.6430e+00, -5.2687e-01,  4.0461e-01,\n",
      "         -1.3222e+00, -3.4549e-01,  1.0298e+00, -1.0876e+00, -8.3444e-01,\n",
      "         -6.0882e-01,  1.3181e+00,  4.9228e-01,  8.8794e-01],\n",
      "        [ 6.1438e-01,  2.0207e+00,  8.8915e-01, -4.2239e-01, -1.8205e+00,\n",
      "          1.3446e+00, -7.4728e-01, -9.3928e-01,  2.6059e-01, -8.0005e-01,\n",
      "          4.1923e-01, -7.1012e-01, -1.0935e+00,  1.0740e+00,  9.4363e-01,\n",
      "          5.3029e-01, -7.8649e-01,  1.1917e+00,  4.0230e-01, -2.7496e-01,\n",
      "          3.3853e-01, -1.2116e-02, -1.3794e+00, -2.4626e-01, -6.9171e-01,\n",
      "          3.4264e-01, -8.4773e-01, -5.4344e-01, -2.6474e-01],\n",
      "        [ 2.3414e+00, -1.1852e-01, -1.6237e-01,  1.2057e+00,  6.7099e-01,\n",
      "          9.5721e-01,  3.8021e-01, -3.0119e-01,  2.2759e-01,  6.6719e-01,\n",
      "          3.7782e-01,  3.1462e-01, -6.1078e-02,  6.1311e-01,  6.8543e-02,\n",
      "          3.1323e+00,  7.6658e-01, -5.2815e-01,  9.4551e-01, -1.0343e+00,\n",
      "          9.7062e-01,  2.2193e-02, -6.3340e-01, -4.5597e-01, -1.0773e-02,\n",
      "          6.6898e-01,  1.3542e+00, -5.2992e-01,  1.4072e+00],\n",
      "        [ 2.0440e+00, -1.5115e+00,  1.0838e+00, -6.5184e-01, -5.3998e-01,\n",
      "         -6.4142e-01, -4.3146e-01, -1.3959e+00, -6.5143e-01, -1.2441e+00,\n",
      "         -1.0206e-01, -1.1709e+00, -2.2602e-01,  2.2167e-01,  3.8640e-01,\n",
      "         -2.4407e+00, -3.0529e-01,  5.4131e-01,  1.2610e+00, -2.8899e-01,\n",
      "         -1.7611e-01,  1.3873e-01, -1.4799e+00, -9.5281e-01, -1.0411e+00,\n",
      "         -4.2865e-01,  7.8110e-01,  4.8464e-02, -3.7738e-01],\n",
      "        [ 9.3064e-01, -2.1918e-02, -4.1165e-01,  2.5171e-01, -9.1370e-01,\n",
      "         -6.8911e-01, -2.0193e+00,  1.6039e-01, -6.9057e-02, -1.4372e+00,\n",
      "         -3.6506e-01, -1.8328e+00,  5.5329e-01, -1.8910e-01,  1.9114e+00,\n",
      "          6.3342e-01,  2.9135e-03,  9.7714e-01,  2.2584e-01,  1.6593e-01,\n",
      "          4.6434e-01, -7.0746e-01,  1.9579e+00, -2.8177e+00,  7.3648e-01,\n",
      "          1.1690e+00,  2.0695e-01, -9.6096e-01, -1.2412e+00],\n",
      "        [ 5.0969e-01,  9.2485e-01,  5.6280e-01, -6.9910e-01, -1.1780e+00,\n",
      "          5.1319e-01,  3.6402e-01,  4.5323e-01, -1.6376e-01, -2.2387e-01,\n",
      "         -8.5450e-01,  5.9325e-01, -6.0732e-01, -1.4611e+00,  1.3977e+00,\n",
      "         -1.4275e+00, -4.0028e-01, -3.1753e-01, -5.0299e-01,  7.5173e-01,\n",
      "         -1.0545e+00,  3.0840e-01,  8.8917e-01,  1.9041e+00, -3.9879e-01,\n",
      "         -1.2683e+00,  5.9541e-01, -1.6207e+00, -9.9728e-01],\n",
      "        [ 3.0398e-02, -2.3294e-01, -4.6425e-02,  1.1468e+00, -6.0764e-01,\n",
      "         -4.7913e-01, -1.6718e+00, -1.1659e+00,  1.9521e-01, -1.8758e+00,\n",
      "         -5.8955e-01,  2.1646e-01, -1.8552e-01,  6.9011e-01,  4.4444e-01,\n",
      "          6.2927e-02,  2.7623e-01, -1.2532e+00,  6.2013e-01,  3.2377e-01,\n",
      "         -4.4896e-01, -1.5083e+00,  2.2332e-01,  2.3274e+00,  1.2327e-01,\n",
      "         -3.6780e-02, -2.0817e+00,  9.8028e-01,  9.4817e-01]]) tensor([ 3, 11,  3, 24,  3,  7,  3,  3]) tensor(3.8597)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/huijie/.cache/torch/hub/adeelh_pytorch-multi-class-focal-loss_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  4,  1, 15,  0, 22, 23, 23])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F \n",
    "focal_loss = torch.hub.load(\n",
    "\t'adeelh/pytorch-multi-class-focal-loss',\n",
    "\tmodel='FocalLoss',\n",
    "\t# alpha=torch.tensor([.75, .25]),\n",
    "\tgamma=2,\n",
    "\treduction='mean',\n",
    "\tforce_reload=False\n",
    ")\n",
    "y = F.one_hot(label,num_classes=29)\n",
    "print(y)\n",
    "x, y = torch.randn(8, 29), label\n",
    "print(x.shape,y.shape)\n",
    "loss = focal_loss(x, y)\n",
    "print(x,y,loss)\n",
    "torch.argmax(F.softmax(x,dim =-1),dim =-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  torch.Size([8, 512])\n",
      "attention_mask:  torch.Size([8, 512])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0807,  0.1494,  0.0778,  ..., -0.1175,  0.1036,  0.0192],\n",
      "         [-0.0732,  0.0051, -0.0360,  ...,  0.2372,  0.0604, -0.0624],\n",
      "         [ 0.0651,  0.1328, -0.0253,  ..., -0.1565, -0.0925,  0.0021],\n",
      "         ...,\n",
      "         [ 0.0718,  0.1522,  0.0205,  ..., -0.2209,  0.0358,  0.0645],\n",
      "         [ 0.0718,  0.1522,  0.0205,  ..., -0.2209,  0.0358,  0.0645],\n",
      "         [ 0.0718,  0.1522,  0.0205,  ..., -0.2209,  0.0358,  0.0645]],\n",
      "\n",
      "        [[ 0.0847,  0.1457,  0.0725,  ..., -0.1094,  0.0993,  0.0049],\n",
      "         [-0.0308, -0.0061, -0.0442,  ...,  0.2323,  0.0551, -0.0845],\n",
      "         [ 0.0589,  0.1275, -0.0280,  ..., -0.1733, -0.0806, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0773,  0.1486,  0.0145,  ..., -0.2129,  0.0300,  0.0485],\n",
      "         [ 0.0773,  0.1486,  0.0145,  ..., -0.2129,  0.0300,  0.0485],\n",
      "         [ 0.0773,  0.1486,  0.0145,  ..., -0.2129,  0.0300,  0.0485]],\n",
      "\n",
      "        [[ 0.1003,  0.1111,  0.0859,  ..., -0.1123,  0.0976,  0.0094],\n",
      "         [-0.0257,  0.0405, -0.0446,  ...,  0.2064,  0.0764,  0.0312],\n",
      "         [-0.0886,  0.0629,  0.0318,  ..., -0.1545,  0.0065,  0.2265],\n",
      "         ...,\n",
      "         [ 0.0914,  0.1035,  0.0161,  ..., -0.2389,  0.0104,  0.0639],\n",
      "         [ 0.0914,  0.1035,  0.0161,  ..., -0.2389,  0.0104,  0.0639],\n",
      "         [ 0.0914,  0.1035,  0.0161,  ..., -0.2389,  0.0104,  0.0639]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0900,  0.1406,  0.0779,  ..., -0.1020,  0.1001,  0.0005],\n",
      "         [ 0.1121,  0.0322, -0.0493,  ..., -0.1239,  0.0627,  0.1208],\n",
      "         [-0.0300,  0.0068, -0.0131,  ...,  0.1595,  0.0466,  0.1811],\n",
      "         ...,\n",
      "         [ 0.0784,  0.1378,  0.0056,  ..., -0.2253,  0.0114,  0.0531],\n",
      "         [ 0.0784,  0.1378,  0.0056,  ..., -0.2253,  0.0114,  0.0531],\n",
      "         [ 0.0784,  0.1378,  0.0056,  ..., -0.2253,  0.0114,  0.0531]],\n",
      "\n",
      "        [[ 0.0837,  0.1704,  0.0892,  ..., -0.1181,  0.1107,  0.0076],\n",
      "         [-0.0191,  0.0385,  0.0199,  ..., -0.0349,  0.0014, -0.1220],\n",
      "         [ 0.0606,  0.0414,  0.0443,  ..., -0.0585,  0.0357,  0.1700],\n",
      "         ...,\n",
      "         [ 0.0758,  0.1799,  0.0213,  ..., -0.2500,  0.0257,  0.0638],\n",
      "         [ 0.0758,  0.1799,  0.0213,  ..., -0.2500,  0.0257,  0.0638],\n",
      "         [ 0.0758,  0.1799,  0.0213,  ..., -0.2500,  0.0257,  0.0638]],\n",
      "\n",
      "        [[ 0.0871,  0.1760,  0.0888,  ..., -0.1199,  0.1138,  0.0035],\n",
      "         [-0.0177,  0.0425,  0.0270,  ..., -0.0334, -0.0007, -0.1227],\n",
      "         [ 0.0635,  0.0482,  0.0440,  ..., -0.0601,  0.0310,  0.1702],\n",
      "         ...,\n",
      "         [ 0.0784,  0.1891,  0.0081,  ..., -0.2770,  0.0140,  0.0687],\n",
      "         [ 0.0784,  0.1891,  0.0081,  ..., -0.2770,  0.0140,  0.0687],\n",
      "         [ 0.0784,  0.1891,  0.0081,  ..., -0.2770,  0.0140,  0.0687]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-2.8094e-02,  2.4142e-01,  1.1295e-01,  ..., -4.6428e-02,\n",
      "         -1.0459e-01,  1.1801e-01],\n",
      "        [-2.7040e-02,  2.4516e-01,  1.1328e-01,  ..., -3.6680e-02,\n",
      "         -1.0226e-01,  1.1901e-01],\n",
      "        [-2.7024e-02,  2.3221e-01,  1.1873e-01,  ..., -2.9578e-02,\n",
      "         -9.9633e-02,  1.1798e-01],\n",
      "        ...,\n",
      "        [-2.9022e-02,  2.3178e-01,  1.1066e-01,  ..., -4.1495e-02,\n",
      "         -1.0824e-01,  1.1272e-01],\n",
      "        [-3.0579e-02,  2.1770e-01,  1.0335e-01,  ..., -1.3644e-02,\n",
      "         -1.2503e-01,  1.0057e-01],\n",
      "        [-3.3964e-02,  2.1059e-01,  9.9550e-02,  ..., -1.0515e-04,\n",
      "         -1.3002e-01,  9.4253e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    label = batch.pop(\"labels\")\n",
    "    for key,item in batch.items():\n",
    "        print(key+\": \" ,item.shape)\n",
    "    out = model(**batch)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f43e34e48e0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(vaild,batch_size=4,collate_fn=con_fun) \n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:705\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=703'>704</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=704'>705</a>\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=706'>707</a>\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=707'>708</a>\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=708'>709</a>\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=709'>710</a>\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=710'>711</a>\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=711'>712</a>\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/huijie/legal/mymodel/test.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/mymodel/test.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/mymodel/test.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m eval_dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/mymodel/test.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m     a \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/mymodel/test.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(a)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py:221\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=219'>220</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=220'>221</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=221'>222</a>\u001b[0m         features,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=222'>223</a>\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=223'>224</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=224'>225</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=225'>226</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=226'>227</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=227'>228</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/data/data_collator.py?line=228'>229</a>\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2795\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2791'>2792</a>\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2792'>2793</a>\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m-> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2794'>2795</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=205'>206</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=207'>208</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=209'>210</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=715'>716</a>\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=716'>717</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=717'>718</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=718'>719</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=719'>720</a>\u001b[0m             )\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=720'>721</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=721'>722</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=722'>723</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=723'>724</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=725'>726</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    a = batch.pop(\"classification\")\n",
    "    print(a)\n",
    "    print(batch)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( precision    recall  f1-score   support\\n\\n           0       0.48      1.00      0.65        16\\n           1       0.00      0.00      0.00         1\\n           2       0.00      0.00      0.00         1\\n           3       0.00      0.00      0.00         3\\n           4       0.33      1.00      0.50         2\\n           5       0.54      0.88      0.67         8\\n           6       0.50      1.00      0.67         8\\n           7       0.00      0.00      0.00         6\\n           8       0.00      0.00      0.00         1\\n           9       0.00      0.00      0.00         2\\n          11       0.00      0.00      0.00         3\\n          12       0.00      0.00      0.00         3\\n          13       0.00      0.00      0.00         1\\n          14       0.00      0.00      0.00         1\\n          20       0.00      0.00      0.00         1\\n          21       0.00      0.00      0.00         2\\n          22       0.00      0.00      0.00         4\\n          26       0.00      0.00      0.00         1\\n          28       0.00      0.00      0.00         4\\n\\n    accuracy                           0.49        68\\n   macro avg       0.10      0.20      0.13        68\\nweighted avg       0.25      0.49      0.33        68\\n)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344c146f3d74f7efd360abb5a7510a11c28372d218d34b58ffcf5f0c0e1377b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hugface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
