{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "import config\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "#example = \"(ii)   If so, whether it was reasonable for P to continue its action against D1 after receiving the Joint Report and/or after the withdrawal of contribution notice against D1 by D2?\"\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(example):\n",
    "    tmp = re.sub(r'^(\\d+).', '', example)\n",
    "    tmp = re.sub(r'^\\((ix|iv|v?i{0,3})\\)', '', tmp)\n",
    "    tmp = re.sub(r'^\\((\\d+)\\)', '', tmp)\n",
    "    new_str = tmp.translate(str.maketrans('','',string.punctuation)).replace(\"’s\",'').replace(\"s’\",'s')\n",
    "    return new_str\n",
    "\n",
    "# date prepared for training\n",
    "def tokenize_function(examples):\n",
    "    lower = [clean_data(x.lower()) for x in examples[\"paragraphs\"]]\n",
    "    result = tokenizer(lower)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // config.chunk_size) * config.chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + config.chunk_size] for i in range(0, total_length, config.chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "## data loader\n",
    "def getDataloader(hklii_dataset,eval_dataset):\n",
    "    train_dataloader = DataLoader(\n",
    "        hklii_dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        batch_size=config.batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, batch_size=config.batch_size, collate_fn=default_data_collator\n",
    "    )\n",
    "    return train_dataloader,eval_dataloader\n",
    "\n",
    "def getDataset():\n",
    "    tokenized_datasets=load_from_disk(\"/home/huijie/legal/huggface/data_prepare/HKLII_all_cail/\" )\n",
    "\n",
    "    # tokenized_datasets = hklii_dataset.map(\n",
    "    #     tokenize_function, batched=True, remove_columns=[\"ID\",\"topic\",\"paragraphs\"],num_proc =16\n",
    "    # )\n",
    "    hklii_dataset = tokenized_datasets.map(group_texts, batched=True,num_proc = 16)\n",
    "\n",
    "    # train_size = 10_000\n",
    "    # test_size = int(0.1 * train_size)\n",
    "\n",
    "    # hklii_dataset = hklii_dataset[\"train\"].train_test_split(\n",
    "    #     train_size=train_size, test_size=test_size, seed=42\n",
    "    # )\n",
    "\n",
    "\n",
    "    hklii_dataset = hklii_dataset.remove_columns([\"word_ids\"])\n",
    "\n",
    "    eval_dataset = hklii_dataset[\"test\"].map(\n",
    "        insert_random_mask,\n",
    "        batched=True,\n",
    "        remove_columns=hklii_dataset[\"test\"].column_names,\n",
    "    )\n",
    "    eval_dataset = eval_dataset.rename_columns(\n",
    "        {\n",
    "            \"masked_input_ids\": \"input_ids\",\n",
    "            \"masked_attention_mask\": \"attention_mask\",\n",
    "            \"masked_labels\": \"labels\",\n",
    "        }\n",
    "    )\n",
    "    eval_dataset =  eval_dataset.remove_columns([\"masked_token_type_ids\"])\n",
    "    return hklii_dataset,eval_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a7bb499a86560565\n",
      "Reusing dataset json (/home/huijie/.cache/huggingface/datasets/json/default-a7bb499a86560565/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dc9f89a2494a1fbfd2cdc1ebd78315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json',data_files='/home/huijie/legal/cail_scm_2/data/raw/CAIL2019-SCM-big/SCM_5k.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['B', 'A', 'label', 'C'],\n",
       "        num_rows: 5102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Cannot execute code, session has been disposed. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def augment_scm(example):\n",
    "    outputs = []\n",
    "    df_cp1 = example.copy()\n",
    "    df_cp1[\"B\"] = example[\"C\"]\n",
    "    df_cp1[\"C\"] = example[\"B\"]\n",
    "    df_cp1[\"label\"] = \"C\" if example['label'] == \"B\" else \"B\"\n",
    "\n",
    "    # 自反性增广\n",
    "    df_cp2 = example.copy()\n",
    "    df_cp2[\"A\"] = example[\"C\"]\n",
    "    df_cp2[\"B\"] = example[\"C\"]\n",
    "    df_cp2[\"C\"] = example[\"A\"]\n",
    "    df_cp2[\"label\"] = \"B\"\n",
    "\n",
    "    # 自反性+反对称增广\n",
    "    df_cp3 = example.copy()\n",
    "    df_cp3[\"A\"] = example[\"C\"]\n",
    "    df_cp3[\"B\"] = example[\"A\"]\n",
    "    df_cp3[\"C\"] = example[\"C\"]\n",
    "    df_cp3[\"label\"] = \"C\"\n",
    "\n",
    "    # 启发式增广\n",
    "    df_cp4 = example.copy()\n",
    "    if example[\"label\"] == \"B\":\n",
    "        df_cp4[\"A\"] = example[\"B\"]\n",
    "        df_cp4[\"B\"] = example[\"A\"]\n",
    "        df_cp4[\"C\"] = example[\"C\"]\n",
    "        df_cp4[\"label\"] = \"B\"\n",
    "    else:\n",
    "        df_cp4[\"A\"] = example[\"C\"]\n",
    "        df_cp4[\"B\"] = example[\"B\"]\n",
    "        df_cp4[\"C\"] = example[\"A\"]\n",
    "        df_cp4[\"label\"] = \"C\"\n",
    "\n",
    "    # 启发式+反对称增广\n",
    "    df_cp5 = example.copy()\n",
    "    if example[\"label\"] == \"B\":\n",
    "        df_cp5[\"A\"] = example[\"B\"]\n",
    "        df_cp5[\"B\"] = example[\"C\"]\n",
    "        df_cp5[\"C\"] = example[\"A\"]\n",
    "        df_cp5[\"label\"] = \"C\"\n",
    "    else:\n",
    "        df_cp5[\"A\"] = example[\"C\"]\n",
    "        df_cp5[\"B\"] = example[\"A\"]\n",
    "        df_cp5[\"C\"] = example[\"B\"]\n",
    "        df_cp5[\"label\"] = \"B\"\n",
    "    outputs+= [example, df_cp1, df_cp2, df_cp3, df_cp4, df_cp5]\n",
    "    return {'data': outputs}\n",
    "\n",
    "augment_scm(dataset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset = dataset['train'].map(augment_scm,num_proc=4)\n",
    "augmented_dataset[:9]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:705\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=703'>704</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=704'>705</a>\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=706'>707</a>\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=707'>708</a>\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=708'>709</a>\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=709'>710</a>\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=710'>711</a>\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=711'>712</a>\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 9 at dim 1 (got 8)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/huijie/legal/cail_scm_2/test.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/cail_scm_2/test.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/cail_scm_2/test.ipynb#ch0000006vscode-remote?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/cail_scm_2/test.ipynb#ch0000006vscode-remote?line=6'>7</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer([\u001b[39m\"\u001b[39;49m\u001b[39mHello, my dog is cute\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mNo, it is not.\u001b[39;49m\u001b[39m\"\u001b[39;49m], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/cail_scm_2/test.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/cail_scm_2/test.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m last_hidden_states \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2403\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2398'>2399</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2399'>2400</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2400'>2401</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2401'>2402</a>\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2402'>2403</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2403'>2404</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2404'>2405</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2405'>2406</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2406'>2407</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2407'>2408</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2408'>2409</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2409'>2410</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2410'>2411</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2411'>2412</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2412'>2413</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2413'>2414</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2414'>2415</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2415'>2416</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2416'>2417</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2417'>2418</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2418'>2419</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2419'>2420</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2420'>2421</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2421'>2422</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2422'>2423</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2423'>2424</a>\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2424'>2425</a>\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2440'>2441</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2441'>2442</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2588\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2577'>2578</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2578'>2579</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2579'>2580</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2580'>2581</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2584'>2585</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2585'>2586</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2587'>2588</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2588'>2589</a>\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2589'>2590</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2590'>2591</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2591'>2592</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2592'>2593</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2593'>2594</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2594'>2595</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2595'>2596</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2596'>2597</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2597'>2598</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2598'>2599</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2599'>2600</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2600'>2601</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2601'>2602</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2602'>2603</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2603'>2604</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2604'>2605</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2605'>2606</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py:694\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=690'>691</a>\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=691'>692</a>\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=693'>694</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=694'>695</a>\u001b[0m     input_ids,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=695'>696</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=696'>697</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=697'>698</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=698'>699</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=699'>700</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=700'>701</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=701'>702</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=702'>703</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=703'>704</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=704'>705</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=705'>706</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=706'>707</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=707'>708</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=708'>709</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=710'>711</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py:774\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=763'>764</a>\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=765'>766</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=766'>767</a>\u001b[0m     batch_outputs,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=767'>768</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=770'>771</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=771'>772</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=773'>774</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=775'>776</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=205'>206</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=207'>208</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=209'>210</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=715'>716</a>\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=716'>717</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=717'>718</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=718'>719</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=719'>720</a>\u001b[0m             )\n\u001b[0;32m--> <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=720'>721</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=721'>722</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=722'>723</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=723'>724</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=725'>726</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute\",\"No, it is not.\"], return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 29155,   117, 11153, 14791, 10127, 18233, 10111,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344c146f3d74f7efd360abb5a7510a11c28372d218d34b58ffcf5f0c0e1377b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hugface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
