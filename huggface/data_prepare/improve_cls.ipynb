{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import stanza\n",
    "\n",
    "# filter the length < 7\n",
    "def clean_data(example):\n",
    "    tmp = re.sub(r'^(\\d+).', '', example)\n",
    "    tmp = re.sub(r'^\\((ix|iv|v?i{0,3})\\)', '', tmp)\n",
    "    tmp = re.sub(r'^\\((\\d+)\\)', '', tmp)\n",
    "    new_str = tmp.translate(str.maketrans('','',string.punctuation)).replace(\"’s\",'').replace(\"s’\",'s')\n",
    "    return new_str\n",
    "\n",
    "# date prepared for training\n",
    "def tokenize_function(examples):\n",
    "    lower = [clean_data(x.lower()) for x in examples[\"paragraphs\"]]\n",
    "    lower = filter(lambda x : len(x) >7, lower )\n",
    "    result = tokenizer(lower)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# split the paragraphs to sentence\n",
    "# [(sent1,sent2,isnext)]\n",
    "# def paragraphs_split(example):\n",
    "#     nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "#     doc = nlp(example[\"paragraphs\"])\n",
    "#     sentences = [sentence.text for sentence in doc.sentences]\n",
    "#     _get_nsp_data_from_paragraph\n",
    "\n",
    "\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # `paragraphs` is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next\n",
    "\n",
    "\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, max_len=512):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "hklii_dataset_ch = load_dataset(\"json\", data_files=\"/home/xijia/nlp/data_prepare/data/HKLII_zh.json\")\n",
    "hklii_dataset_ch = hklii_dataset_ch[\"train\"]\n",
    "hklii_dataset_en = load_dataset(\"json\", data_files=\"/home/huijie/legal/huggface/data_prepare/HKLII-online/HKLII.json\")\n",
    "hklii_dataset_en = hklii_dataset_en[\"train\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hklii = train paragraphs sentences\n",
    "\n",
    "1. shuffle paragraphs\n",
    "2. get the setences in paragraph\n",
    "2. for sentences in paragraphs:\n",
    "    for sentence in sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_split(example):\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "    doc = nlp(example[\"paragraphs\"])\n",
    "    sentences = [sentence.text for sentence in doc.sentences]\n",
    "    example[\"sentences\"] = sentences\n",
    "    return example\n",
    "sampled = hklii_dataset_en[:10]\n",
    "sampled.map(paragraphs_split,nproc =4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344c146f3d74f7efd360abb5a7510a11c28372d218d34b58ffcf5f0c0e1377b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hugface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
