{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = pd.Series(len_status)\n",
    "# print(s.describe())\n",
    "# len_status[-1]\n",
    "# dic\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build hugface datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "# processin funtion\n",
    "def lowercase_condition(examples):\n",
    "    examples[\"paragraphs\"] = [x.lower() for x in examples[\"paragraphs\"]]\n",
    "    return  examples\n",
    "\n",
    "def filter_nones(x):\n",
    "    return x[\"condition\"] is not None\n",
    "# filter only provide __liter__ __next__ so the return type is filter\n",
    "# map batch== True require the example be list type\n",
    "# Remark: did before turn into datasets\n",
    "# def filter_sentence_length(example):\n",
    "#     print()\n",
    "#     for _ in example[\"paragraphs\"]:\n",
    "\n",
    "#     add_on = list(filter(lambda x: len(x) > 5 ,example[\"paragraphs\"]))\n",
    "#     example[\"paragraphs\"] = add_on \n",
    "#     return example\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"paragraphs\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "def tokenize_function(examples):\n",
    "    lower = [x.lower() for x in examples[\"paragraphs\"]]\n",
    "    result = tokenizer(lower)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['paragraphs', 'topic', 'ID'],\n",
    "        num_rows: 1578212\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['paragraphs', 'topic', 'ID'],\n",
    "        num_rows: 175357\n",
    "    })\n",
    "})\n",
    "\n",
    "mean       75.771333\n",
    "std        51.978543\n",
    "min        20.000000\n",
    "25%        38.000000\n",
    "50%        61.000000\n",
    "75%        97.000000\n",
    "max       435.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a712c258280d2958\n",
      "Reusing dataset json (/home/huijie/.cache/huggingface/datasets/json/default-a712c258280d2958/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641eda749f7f4b9d8610926026b966c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraphs', 'topic', 'ID'],\n",
      "        num_rows: 1458006\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['paragraphs', 'topic', 'ID'],\n",
      "        num_rows: 162001\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hklii_dataset = load_dataset(\"json\", data_files=\"/home/xijia/nlp/data_prepare/data/HKLII_zh.json\")\n",
    "# print(hklii_dataset)\n",
    "#hklii_dataset = hklii_dataset.shuffle(seed=42) # not need for this step\n",
    "# hklii_dataset = hklii_dataset.map(\n",
    "#     lowercase_condition, batched=True,num_proc = 16\n",
    "# )\n",
    "hklii_dataset = hklii_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "print(hklii_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraphs': ['12.  PW1-PW5是在反毒行動中扮作顧客的卧底警員；', '申請人： 由法律援助署委派何柏生馬華潤律師行轉聘大律師孫錦熹代表。', '14'], 'topic': ['HKDC', 'HKCA', 'HKCFI'], 'ID': ['2016_605.json', '2012_1.json', '2002_1338.json']}\n",
      "count    1.458006e+06\n",
      "mean     7.819433e+01\n",
      "std      9.874830e+01\n",
      "min      1.000000e+00\n",
      "25%      1.500000e+01\n",
      "50%      4.400000e+01\n",
      "75%      1.080000e+02\n",
      "max      6.492000e+03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(hklii_dataset[\"train\"][:3])\n",
    "stat = [len(inp) for inp in hklii_dataset[\"train\"][\"paragraphs\"]]\n",
    "import pandas as pd\n",
    "s = pd.Series(stat)\n",
    "print(s.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 113, 123, 114, 4302, 2433, 4316, 2770, 2433, 1651, 2586, 2881, 5975, 1651, 8224, 6566, 3449, 2881, 10032, 1985, 2380, 2074, 6812, 2081, 1968, 109, 31367, 5975, 9661, 9648, 113, 6392, 1732, 4107, 6824, 114, 1483, 102], [101, 113, 1651, 1866, 3246, 5724, 114, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]], 'word_ids': [[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, None], [None, 0, 1, 2, 3, 4, 5, None]]}\n",
      "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-uncased', vocab_size=105879, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "result = tokenize_function(hklii_dataset[\"train\"][1:3])\n",
    "print(result)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 1458006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 162001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = hklii_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"ID\",\"topic\",\"paragraphs\"],num_proc = 16\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 1088'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:20]\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 512'\n",
      "'>>> Chunk length: 512'\n",
      "'>>> Chunk length: 64'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 512\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 198802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 22025\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True,num_proc = 32)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邀 請 他 人 加 入 三 合 會 。 [SEP] [CLS] 14. 裁 判 官 不 給 予 pw1 證 詞 比 重, 鑑 於 裁 判 官 對 證 人 有 耳 聞 目 睹 的 優 勢, 提 出 的 理 由 也 合 理, 本 席 於 考 慮 時 將 pw1 的 證 詞 拼 棄 。 [SEP] [CLS] 10. 10 月 2 日 ， 申 請 人 向 教 育 統 籌 局 投 訴 學 校 處 理 19 / 2 / 04 事 件 不 當 ， 但 沒 有 結 果 。 [SEP] [CLS] d8 的 控 罪 [SEP] [CLS] 判 案 書 [SEP] [CLS] 6. 本 案 申 請 人 的 上 訴 無 理 據 支 持 。 本 庭 駁 回 上 訴 ， 不 作 出 訟 費 命 令 。 [SEP] [CLS] 其 後 ， 警 員 搜 查 那 黑 色 環 保 袋 ， 發 現 一 袋 涉 案 「 k 仔 」 。 根 據 警 方 資 料 ， 這 些 「 k 仔 」 的 零 售 價 約 為 二 萬 九 千 元 。 [SEP] [CLS] 25. 上 述 2004 年 10 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 343. 7 平 方 米 ( 見 文 件 夾 第 86 - 87 頁 ) 。 上 述 2004 年 11 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 1, 188. 9 平 方 米 、 完 成 5 個 沙 井 蓋 及 提 供 3 工 代 工 ( 見 文 件 夾 第 88 頁 ) 。 上 述 2004 年 12 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 3, 886. 95 平 方 米 、 完 成 13 個 沙 井 蓋 及 提 供 6 工 代 工 ( 見 文 件 夾 第 89 - 90 頁 ) 。 [SEP] [CLS] 2. 經 審 訊 後 ， 本 席 於 本 年 6 月 21 日 ， 頒 下 判 案 書 ( 下 簡 稱 [UNK] 判 案 書 [UNK] ) ， 判 周 女 士 敗 訴 ， 撤 銷 了 她 的 申 索 。 按 此 結 果 ， 本 席 同 時 作 出 暫 時 命 令 ， 周 女 士 須 支 付 署 方 因 訴 訟 引 起 的 訟 費 ， 包 括 大 律 師 費 用 。 除 非 雙 方 協 議 ， 訟 費 由 法 庭 評 定 。 雙 方 可 以 於 14 天 内 提 出 申 請 要 求 更 改 訟 費 命 令 ， 否 則 命 令 將 自 動 作 實 。 [SEP] [CLS] dcec 1530 / 2006 [SEP] [CLS] 28. 調 查 公 司 的 報 告 還 有 很 多 其 他\n",
      "邀 請 他 人 加 入 三 合 會 。 [SEP] [CLS] 14. 裁 判 官 不 給 予 pw1 證 詞 比 重, 鑑 於 裁 判 官 對 證 人 有 耳 聞 目 睹 的 優 勢, 提 出 的 理 由 也 合 理, 本 席 於 考 慮 時 將 pw1 的 證 詞 拼 棄 。 [SEP] [CLS] 10. 10 月 2 日 ， 申 請 人 向 教 育 統 籌 局 投 訴 學 校 處 理 19 / 2 / 04 事 件 不 當 ， 但 沒 有 結 果 。 [SEP] [CLS] d8 的 控 罪 [SEP] [CLS] 判 案 書 [SEP] [CLS] 6. 本 案 申 請 人 的 上 訴 無 理 據 支 持 。 本 庭 駁 回 上 訴 ， 不 作 出 訟 費 命 令 。 [SEP] [CLS] 其 後 ， 警 員 搜 查 那 黑 色 環 保 袋 ， 發 現 一 袋 涉 案 「 k 仔 」 。 根 據 警 方 資 料 ， 這 些 「 k 仔 」 的 零 售 價 約 為 二 萬 九 千 元 。 [SEP] [CLS] 25. 上 述 2004 年 10 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 343. 7 平 方 米 ( 見 文 件 夾 第 86 - 87 頁 ) 。 上 述 2004 年 11 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 1, 188. 9 平 方 米 、 完 成 5 個 沙 井 蓋 及 提 供 3 工 代 工 ( 見 文 件 夾 第 88 頁 ) 。 上 述 2004 年 12 月 27 日 已 付 糧 單 顯 示 鋪 磚 面 積 為 3, 886. 95 平 方 米 、 完 成 13 個 沙 井 蓋 及 提 供 6 工 代 工 ( 見 文 件 夾 第 89 - 90 頁 ) 。 [SEP] [CLS] 2. 經 審 訊 後 ， 本 席 於 本 年 6 月 21 日 ， 頒 下 判 案 書 ( 下 簡 稱 [UNK] 判 案 書 [UNK] ) ， 判 周 女 士 敗 訴 ， 撤 銷 了 她 的 申 索 。 按 此 結 果 ， 本 席 同 時 作 出 暫 時 命 令 ， 周 女 士 須 支 付 署 方 因 訴 訟 引 起 的 訟 費 ， 包 括 大 律 師 費 用 。 除 非 雙 方 協 議 ， 訟 費 由 法 庭 評 定 。 雙 方 可 以 於 14 天 内 提 出 申 請 要 求 更 改 訟 費 命 令 ， 否 則 命 令 將 自 動 作 實 。 [SEP] [CLS] dcec 1530 / 2006 [SEP] [CLS] 28. 調 查 公 司 的 報 告 還 有 很 多 其 他\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))\n",
    "\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [\"However, D2 was forced to move out from PW9's premises around the midnight of 6/7 November 2016.\",\"D2 simply had no time to give advance notice.\",\"Though the defendant had indicated his consent earlier, how D2 could ensure he would be allowed entry.\",\"The defendant liked glasses and was ordering glasses for his collection, the burglary of the optical company committed by D2 probably was not a mere coincidence, but to secure his entry into the defendant’s premises.\"] \n",
    "model_inputs = tokenizer(text_pair = sents)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] [MASK] 及 [MASK] 政 [MASK] 訟 案 2007 年 第 78 號 [SEP] [CLS] 判 案 書 日 期 [MASK] 2006 年 12 月 11 日 [SEP] [CLS] 46. 本 席 必 [MASK] 明 確 指 出 [MASK] 父 母 子 女 ， 夫 teorije ， 或 兄 弟 姊 妹 在 家 事 上 之 安 [MASK] 協 議 ， 除 [MASK] 在 事 前 特 別 聲 [MASK] 可 [MASK] 互 雙 追 討 ， 清 楚 顯 明 雙 方 有 建 [MASK] 法 律 關 係 之 意 願 ， 令 該 安 排 協 議 [MASK] 生 法 [MASK] 後 果 ， 否 則 在 法 律 上 沒 有 任 何 約 束 力 。 [SEP] [CLS] 25. 至 於 上 訴 人 שימש 將 連 女 士 的 信 件 交 objekata [MASK] 方 證 人 們 ， 是 不 分 [MASK] 紅 皂 白 ， 抑 或 是 托 詞 ？ 上 訴 人 指 他 回 覆 管 理 處 不 會 將 信 件 交 予 [MASK] 人 [MASK] 會 直 接 交 回 公 司 。 即 使 是 托 辭 不 交 [MASK] [MASK] 是 有 心 為 難 [MASK] 行 為 並 不 合 理 。 潘 大 律 師 指 行 [MASK] 不 [MASK] 理 不 代 表 不 可 信 。 [SEP] [CLS] [UNK] [SEP] [CLS] 32. 經 [MASK] 訊 之 後 ， 被 裁 定 第 十 一 、 第 十 三 、 第 [MASK] 四 及 第 十 [MASK] 項 控 罪槐 cochrane 四 項 控 罪 罪 名 成 立 ， [MASK] 四 項 控 罪 分 別 是 第 十 一 [MASK] 第 十 五 項 控 罪 〈 以 三 合 會 社 團 成 [MASK] 身 分 行 事 〉 、 第 十 三 及 [MASK] 十 四 項 控 [MASK] [MASK] 邀 請 他 人 成 為 [MASK] halbinsel 會 成 [MASK] 〉 [MASK] 所 以 嚴 格 來 說 ， 四 項 控 罪 都 是 與 [MASK] 合 會 有 關 之 餘 [MASK] 就 並 不 是 一 般 最 輕 微 的 聲 稱 ， 因 為 [MASK] 果 一 個 [MASK] 只 是 lograron 玩 ， 聲 稱 「 我 係 三 合 會 份 子 」 的 話 ， 本 席 較 早 時 候 已 經 說 過 ， 可 能 都 得 到 法 庭 [MASK] [MASK] 諒 [MASK] [MASK] 者 [MASK] 受 是 一 個 貪 玩 成 份 ， 所 以 好 多 時 候 聲 稱 三 合 會 [MASK] 控 罪 ， 可 能 最 [MASK] 都 未 必 一 [MASK] 導 致 是 監 禁 式 的 [MASK] 罰 。 但 在 第 六 被 告 人comb 個 案 ， 他 面 對 這 些 [MASK] 三 合 [MASK] 社 團 [MASK] 員 有 關 的 控 罪 ， 全 部 都 是 實 質 [MASK] ， [MASK] [MASK] 有 參 與盘 些 活 動 ， [MASK] 括 [MASK]'\n",
      "\n",
      "'>>> 邀 請 他 人 加 入 三 合 會 。 [SEP] [CLS] [MASK]. 裁 判 官 不 給 予 pw1 證 詞 比 [MASK], 鑑 於 裁 判 官 對 證 人 [MASK] 耳 聞 [MASK] 睹 的 優 [MASK], 提 出 的 理 [MASK] 也 [MASK] 理, 本 [MASK] 於 考 慮 時 將 pw1 [MASK] 證 詞 拼 棄 。 [SEP] [CLS] 10. [MASK] 月 2 [MASK] ， 申 請 人 向 教 育 統 籌 局 投 訴 [MASK] 校 處 理 19 / 2 / 04 事 件 不 當 ， 但 [MASK] 有 結 果 。 [SEP] [CLS] d8 [MASK] 控 罪 [SEP] [CLS] [MASK] [MASK] 書 [SEP] [CLS] 6. 本 案 申 請 人 [MASK] 上 訴 [MASK] 理 據 支 [MASK] 。 本 庭 駁 回 上 訴 ， 不 作 出 訟 費 命 令 。 [SEP] [CLS] 其 [MASK] ， 警 員 搜 查 那 黑 色 環 [MASK] 袋 [MASK] 發 現 一 袋 涉 [MASK] 「 k 仔 」 [MASK] 根 據 警 方 資 [MASK] ， 這 些 「 k [MASK] [MASK] 的 零 售 價 約 為 二 萬 九ᄎ 元 。 [SEP] [CLS] 25. 上 述 2004 年 10 月 27 日 已 付 糧 單 顯 示 鋪 [MASK] 面 積 [MASK] 343. 7 平 方 米 ( 見 文 件 [MASK] 第 86 - 87 頁 ) 。 上 [MASK] [MASK] 年 11 月 27 日 已 付 糧 單 顯 示 鋪 磚 [MASK] [MASK] 為 1, 188. 9 平 方 米 、 完 成 5 個 沙 井 蓋 及 [MASK] 供 3 工 代 工 ( 見 文 件 夾 第 88 頁 ) 。 上 述 2004 channel 12 月 27 日 [MASK] 付 糧 單 顯 示 鋪 磚 [MASK] 積 為 [MASK], 886. 95 平 方 米 、 完 成 13 個 803 井 [MASK] 及 提 供 6 工 代 工 ( 見 文 件 夾 第 89 - 90 [MASK] ) 。 [SEP] [CLS] 2. 經 審 訊 後 ， 本 席 於 本 年 [MASK] 月 21 日 ， 頒 下 判 [MASK] [MASK] ( 下 簡 稱 [UNK] 判 [MASK] 書 [UNK] ) ， 判 周 女 士 敗 訴 ， 撤 銷 了 她 的 申 [MASK] 。 按 此 結 果 ， 本 席 同 妻 作 出 暫 時 命 令 ， [MASK] 女 士 須 支 付 署 [MASK] [MASK] 訴 訟 引 起 的 訟 費 ， [MASK] 括 大 律 師 費 用 。 除 非 雙 [MASK] 協 議 ， 訟 費 由 [MASK] 庭 評 定 [MASK] 雙 方 可 [MASK] 於 14 [MASK] 内 提 出 申 請 要 [MASK] 更 改 訟 費 命 令 [MASK] 否 則 命 令 將 自 動 作 [MASK] 。 [SEP] [CLS] dcec 1530 / 2006 [SEP] [CLS] 28. 調 查 公 司 的 [MASK] 告 還 有 [MASK] 多 其 他'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at zlucia/custom-legalbert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"zlucia/custom-legalbert\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huijie/miniconda3/envs/hugface/lib/python3.8/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "model_name = \"hklii\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/huijie/legal/huggface/data_prepare/test.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000024vscode-remote?line=2'>3</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000024vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>>> Perplexity: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(eval_results[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000024vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab825d2a4d4e4cb3bcd3ec203ce0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/phjhk/HKLII'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_url = create_repo(name=\"HKLII\", repo_type=\"dataset\")\n",
    "repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9120e27b680476f8b51db6c15ede6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1579 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ecfe8f78f9406bab481cc9e22cb32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/176 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hklii_dataset.save_to_disk(\"/home/huijie/legal/huggface/data_prepare/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/datasets/phjhk/HKLII into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'datasets-issues-with-comments.jsonl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"/home/huijie/legal/huggface/data_prepare/HKLII-online\", clone_from=\"https://huggingface.co/datasets/phjhk/HKLII\")\n",
    "!cp datasets-issues-with-comments.jsonl github-issues/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## play with the tokenlizer and try to slove the unk issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['paragraphs'],\n",
       "        num_rows: 1578212\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['paragraphs'],\n",
       "        num_rows: 175357\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hklii_dataset = hklii_dataset.map(\n",
    "    lowercase_condition, batched=True, remove_columns=[\"ID\",\"topic\"],num_proc = 16\n",
    ")\n",
    "hklii_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.  經審訊後，本席於本年6月21日，頒下判案書(下簡稱“判案書”)，判周女士敗訴，撤銷了她的申索。按此結果，本席同時作出暫時命令，周女士須支付署方因訴訟引起的訟費，包括大律師費用。除非雙方協議，訟費由法庭評定。雙方可以於14天内提出申請要求更改訟費命令，否則命令將自動作實。\n",
      "2 . 經 審 訊 後 ， 本 席 於 本 年 6 月 21 日 ， 頒 下 判 案 書 ( 下 簡 稱 [UNK] 判 案 書 [UNK] ) ， 判 周 女 士 敗 訴 ， 撤 銷 了 她 的 申 索 。 按 此 結 果 ， 本 席 同 時 作 出 暫 時 命 令 ， 周 女 士 須 支 付 署 方 因 訴 訟 引 起 的 訟 費 ， 包 括 大 律 師 費 用 。 除 非 雙 方 協 議 ， 訟 費 由 法 庭 評 定 。 雙 方 可 以 於 14 天 内 提 出 申 請 要 求 更 改 訟 費 命 令 ， 否 則 命 令 將 自 動 作 實 。\n"
     ]
    }
   ],
   "source": [
    "example = hklii_dataset[\"train\"]['paragraphs'][13]\n",
    "print(example)\n",
    "print( \" \".join(tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we know that there are some method to remove unk\n",
    "1. remove the , ' in the \n",
    "2. add words : may improve the performance and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總結\n",
      "總結\n",
      "總 結\n",
      "___________________\n",
      "1.  被告擅闖民居，進行盜竊。本席採納一般的入獄3年，為判刑起點。\n",
      "  被告擅闖民居，進行盜竊。本席採納一般的入獄3年，為判刑起點。\n",
      "被 告 擅 闖 民 居 ， 進 行 盜 竊 。 本 席 採 納 一 般 的 入 獄 3 年 ， 為 判 刑 起 點 。\n",
      "___________________\n",
      "27.  答辯人若已準備向申請人提出申索,倘若法院信納答辯人的理據並判決答辯人勝數,答辯人亦理應獲判其損失的賠償, 所以不會因收樓令而造成不能彌補的事實。\n",
      "  答辯人若已準備向申請人提出申索倘若法院信納答辯人的理據並判決答辯人勝數答辯人亦理應獲判其損失的賠償 所以不會因收樓令而造成不能彌補的事實。\n",
      "答 辯 人 若 已 準 備 向 申 請 人 提 出 申 索 倘 若 法 院 信 納 答 辯 人 的 理 據 並 判 決 答 辯 人 勝 數 答 辯 人 亦 理 應 獲 判 其 損 失 的 賠 償 所 以 不 會 因 收 樓 令 而 造 成 不 能 彌 補 的 事 實 。\n",
      "___________________\n",
      "6. 本案申請人的上訴無理據支持。本庭駁回上訴，不作出訟費命令。\n",
      " 本案申請人的上訴無理據支持。本庭駁回上訴，不作出訟費命令。\n",
      "本 案 申 請 人 的 上 訴 無 理 據 支 持 。 本 庭 駁 回 上 訴 ， 不 作 出 訟 費 命 令 。\n",
      "___________________\n",
      "頒下判案書日期 ： 2001年9月28日\n",
      "頒下判案書日期 ： 2001年9月28日\n",
      "頒 下 判 案 書 日 期 ： 2001 年 9 月 28 日\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "#example = \"(ii)   If so, whether it was reasonable for P to continue its action against D1 after receiving the Joint Report and/or after the withdrawal of contribution notice against D1 by D2?\"\n",
    "\n",
    "# print(new_str)\n",
    "# print( \" \".join(tokenizer.tokenize(new_str)))\n",
    "\n",
    "\n",
    "def clean_data(example):\n",
    "    tmp = re.sub(r'^(\\d+).', '', example)\n",
    "    tmp = re.sub(r'^\\((ix|iv|v?i{0,3})\\)', '', tmp)\n",
    "    tmp = re.sub(r'^\\((\\d+)\\)', '', tmp)\n",
    "    new_str = tmp.translate(str.maketrans('','',string.punctuation)).replace(\"’s\",'').replace(\"s’\",'s')\n",
    "    return new_str\n",
    "\n",
    "for i in hklii_dataset_ch[\"train\"]['paragraphs'][15:20]:\n",
    "    clean = clean_data(i)\n",
    "    print(i)\n",
    "    print(clean)\n",
    "    print( \" \".join(tokenizer.tokenize(clean)))\n",
    "    print(\"___________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build chinese and chinese-english datasets togeter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a712c258280d2958\n",
      "Reusing dataset json (/home/huijie/.cache/huggingface/datasets/json/default-a712c258280d2958/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e908d7c0c4174a2a863846c7689dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['paragraphs', 'topic', 'ID'],\n",
      "    num_rows: 1620007\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hklii_dataset_ch = load_dataset(\"json\", data_files=\"/home/xijia/nlp/data_prepare/data/HKLII_zh.json\")\n",
    "hklii_dataset_ch = hklii_dataset_ch[\"train\"]\n",
    "print(hklii_dataset_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-59ce93af8ba84fa0\n",
      "Reusing dataset json (/home/huijie/.cache/huggingface/datasets/json/default-59ce93af8ba84fa0/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88969c324def426d8e65f6882937823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['paragraphs', 'topic', 'ID'],\n",
      "    num_rows: 1753569\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hklii_dataset_en = load_dataset(\"json\", data_files=\"/home/huijie/legal/huggface/data_prepare/HKLII-online/HKLII.json\")\n",
    "hklii_dataset_en = hklii_dataset_en[\"train\"]\n",
    "print(hklii_dataset_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the length < 5 \n",
    "def clean_data(example):\n",
    "    tmp = re.sub(r'^(\\d+).', '', example)\n",
    "    tmp = re.sub(r'^\\((ix|iv|v?i{0,3})\\)', '', tmp)\n",
    "    tmp = re.sub(r'^\\((\\d+)\\)', '', tmp)\n",
    "    new_str = tmp.translate(str.maketrans('','',string.punctuation)).replace(\"’s\",'').replace(\"s’\",'s')\n",
    "    return new_str\n",
    "\n",
    "# date prepared for training\n",
    "def tokenize_function(examples):\n",
    "    lower = [clean_data(x.lower()) for x in examples[\"paragraphs\"]]\n",
    "    lower = filter(lambda x : len(x) >7, lower )\n",
    "    result = tokenizer(lower)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['paragraphs', 'topic', 'ID'],\n",
      "    num_rows: 3373576\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "assert hklii_dataset_en.features.type == hklii_dataset_ch.features.type\n",
    "hklii_dataset = concatenate_datasets([hklii_dataset_en, hklii_dataset_ch])\n",
    "print(hklii_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hklii_dataset = hklii_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "tokenized_datasets = hklii_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"ID\",\"topic\",\"paragraphs\"],num_proc = 8\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hklii_dataset.save_to_disk(\"/home/huijie/legal/huggface/data_prepare/HKLII_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = [len(x) for x in tokenized_datasets[\"train\"][\"input_ids\"][:100] if len(x) > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.036218e+06\n",
      "mean     2.645207e+02\n",
      "std      3.038398e+02\n",
      "min      1.000000e+00\n",
      "25%      4.800000e+01\n",
      "50%      1.750000e+02\n",
      "75%      3.750000e+02\n",
      "max      5.867500e+04\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "stat = [len(inp) for inp in hklii_dataset[\"train\"][\"paragraphs\"]]\n",
    "import pandas as pd\n",
    "s = pd.Series(stat)\n",
    "print(s.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.prediction_logits\n",
    "seq_relationship_logits = outputs.seq_relationship_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/huijie/legal/huggface/data_prepare/test.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000039vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000039vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction_logits\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blpk-gpu1.cs.hku.hk/home/huijie/legal/huggface/data_prepare/test.ipynb#ch0000039vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(seq_relationship_logits\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(**input)\n",
    "print(prediction_logits.shape)\n",
    "print(seq_relationship_logits.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344c146f3d74f7efd360abb5a7510a11c28372d218d34b58ffcf5f0c0e1377b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hugface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
